# -*- coding: utf-8 -*-
"""mjkan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1adHY-L4kMM7qM3fcxqEuKye0MyFH6Q7m
"""

class MJKANLayer(nn.Module):
    def __init__(self, in_dim, out_dim, num_basis=9,
                 grid_min=-1.0, grid_max=1.0, sigma=0.3,
                 use_base_update=True):
        super().__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.sigma = sigma
        # RBF centers
        centers = torch.linspace(grid_min, grid_max, num_basis)
        self.register_buffer("centers", centers)
        # learnable gamma & beta weights per basis
        self.gamma_w = nn.Parameter(torch.randn(in_dim, num_basis, out_dim) * 0.05)
        self.beta_w  = nn.Parameter(torch.randn(in_dim, num_basis, out_dim) * 0.05)
        # base update linear layer
        self.use_base_update = use_base_update
        if use_base_update:
            self.base_activation = F.silu
            self.base_linear = nn.Linear(in_dim, out_dim)

    def forward(self, x):
        # x: [B, in_dim]
        # compute radial basis
        r = (x.unsqueeze(-1) - self.centers).abs()            # [B, in_dim, K]
        basis = torch.exp(-r.pow(2) / (2 * self.sigma**2))   # [B, in_dim, K]
        # compute FiLM parameters
        gamma = torch.einsum("bik,iko->bio", basis, self.gamma_w)  # [B, in_dim, out_dim]
        beta  = torch.einsum("bik,iko->bio", basis, self.beta_w)   # [B, in_dim, out_dim]
        # apply FiLM modulation and sum over input dim
        out = (gamma * x.unsqueeze(-1) + beta).sum(dim=1)    # [B, out_dim]
        # base update
        if self.use_base_update:
            out = out + self.base_linear(self.base_activation(x))
        return out